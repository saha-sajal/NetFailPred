{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317c1e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common data manipulation imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#some other imports\n",
    "import os\n",
    "pd.set_option('display.max_rows', 100000)\n",
    "\n",
    "# For stable output on each run\n",
    "np.random.seed(3)\n",
    "\n",
    "# To plot easily visible pictures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29684105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix, precision_recall_fscore_support\n",
    "import csv\n",
    "from statistics import mean\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "#normalizing the data\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from pandas import DataFrame\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.svm import SVC # Support Vector Machine, two types of kernels\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a847ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train, test):\n",
    "    train_max = train.loc[:, train.columns != 'target']\n",
    "    test_max = test.loc[:, test.columns != 'target']\n",
    "    Y_train = train.loc[:, train.columns == 'target']\n",
    "    Y_test = test.loc[:, test.columns == 'target']\n",
    "    scaler = StandardScaler()\n",
    "    train_p = scaler.fit_transform(train_max)#.loc[:, train.columns!='Class'])\n",
    "    train_p = DataFrame(train_p)\n",
    "    train_p.columns = train_max.columns.values\n",
    "    test_p = scaler.transform(test_max)#.loc[:, test.columns!='Class'])\n",
    "    test_p = DataFrame(test_p)\n",
    "    test_p.columns = test_max.columns.values\n",
    "    \n",
    "    return train_p, Y_train, test_p, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8dbb0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "original = pd.read_csv('200_nodes.csv')\n",
    "original = original.drop(['date', 'name'], axis=1)\n",
    "train, test = train_test_split(original, test_size=0.2)\n",
    "train_original = train.copy()\n",
    "test_original = test.copy()\n",
    "train_s, Y_train, test_s, Y_test = normalize(train, test)\n",
    "len(test_s.columns)\n",
    "\n",
    "smote = SMOTE()\n",
    "# Applying SMOTE on the data\n",
    "train_s, Y_train = smote.fit_resample(train_s, Y_train)\n",
    "len(test_s.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3ceec",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f45737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the memory_profiler package\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb83f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  162.1067090034485 seconds\n",
      "peak memory: 7030.39 MiB, increment: 1737.53 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "def train_model(X, y):\n",
    "    lr = LogisticRegression(solver = 'saga', random_state = 0)\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    lr.fit(X, y)\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return lr\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model(train_s, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04f2866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8647    0    0  434    0    1    0    2    0    0]\n",
      " [   0  624    0    0    0    4    0    0    0    0]\n",
      " [   0    0  457    0    0    0    0    0    0    0]\n",
      " [ 119    0    3 2252    8    1  182   84  172  104]\n",
      " [   0    0    0    1  217    0    0    0    0   11]\n",
      " [   0    3    0    0    0  401    0    0    0    0]\n",
      " [   3    1    0  124    0    0 5136    0    2    0]\n",
      " [  10    0    0  219    1    0    0 9339    2    6]\n",
      " [   2    1    0  360    3    0    0    0 6435    0]\n",
      " [   2    0    0  189   90    0    0    0    0 8148]]\n",
      "Accouracy:  0.9510502283105022\n",
      "Averages: \n",
      "Precision: 0.9180176163048538\n",
      "Recall: 0.9518920274379202\n",
      "F1 score: 0.9319852971911045\n",
      "Category 0:\n",
      "\tPrecision: 0.9845155413867699\n",
      "\tRecall: 0.9518934390136504\n",
      "\tF1 score: 0.9679297028040522\n",
      "Category 1:\n",
      "\tPrecision: 0.9920508744038156\n",
      "\tRecall: 0.9936305732484076\n",
      "\tF1 score: 0.9928400954653938\n",
      "Category 2:\n",
      "\tPrecision: 0.9934782608695653\n",
      "\tRecall: 1.0\n",
      "\tF1 score: 0.9967284623773174\n",
      "Category 3:\n",
      "\tPrecision: 0.6292260407935177\n",
      "\tRecall: 0.7699145299145299\n",
      "\tF1 score: 0.6924969249692497\n",
      "Category 4:\n",
      "\tPrecision: 0.6802507836990596\n",
      "\tRecall: 0.9475982532751092\n",
      "\tF1 score: 0.791970802919708\n",
      "Category 5:\n",
      "\tPrecision: 0.9852579852579852\n",
      "\tRecall: 0.9925742574257426\n",
      "\tF1 score: 0.9889025893958077\n",
      "Category 6:\n",
      "\tPrecision: 0.9657766077472734\n",
      "\tRecall: 0.9753133308013673\n",
      "\tF1 score: 0.9705215419501134\n",
      "Category 7:\n",
      "\tPrecision: 0.9908753315649867\n",
      "\tRecall: 0.9751487939855905\n",
      "\tF1 score: 0.9829491632459741\n",
      "Category 8:\n",
      "\tPrecision: 0.9733777038269551\n",
      "\tRecall: 0.9461843846493163\n",
      "\tF1 score: 0.9595884282731881\n",
      "Category 9:\n",
      "\tPrecision: 0.9853670334986092\n",
      "\tRecall: 0.9666627120654881\n",
      "\tF1 score: 0.9759252605102406\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9b1e9",
   "metadata": {},
   "source": [
    "# 2. K-Nearest Neighbours (K-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41aa0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model_knn(X, y):\n",
    "#     knn = KNeighborsClassifier(metric = 'minkowski', p=3)\n",
    "#     # Train model and time it\n",
    "#     start_time = time.time()\n",
    "#     knn.fit(X, y)\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     # Calculate time taken to train\n",
    "#     time_taken = end_time - start_time\n",
    "#     print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "#     return knn\n",
    "\n",
    "# # train the model and measure the memory usage\n",
    "# %memit lr = train_model_knn(train_s, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66b6e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_lr = lr.predict(test_s)\n",
    "# cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "# print(cm_lr)\n",
    "# acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "# print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "# precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "# print(\"Averages: \")\n",
    "# print(\"Precision:\", mean(precision))\n",
    "# print(\"Recall:\", mean(recall))\n",
    "# print(\"F1 score:\", mean(f1_score))\n",
    "# # print the metrics for each category\n",
    "# for i, label in enumerate(lr.classes_):\n",
    "#     print(f\"Category {label}:\")\n",
    "#     print(f\"\\tPrecision: {precision[i]}\")\n",
    "#     print(f\"\\tRecall: {recall[i]}\")\n",
    "#     print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fcbf4b",
   "metadata": {},
   "source": [
    "# SVM: Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abcb5e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  3320.6942369937897 seconds\n",
      "peak memory: 5436.53 MiB, increment: 2693.31 MiB\n"
     ]
    }
   ],
   "source": [
    "def train_model_svm_l(X, y):\n",
    "    svc = SVC(kernel = 'linear', random_state = 0)\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    svc.fit(X, y)\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return svc\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model_svm_l(train_s, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b658ca98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8678    0    0  406    0    0    0    0    0    0]\n",
      " [   0  623    0    0    0    5    0    0    0    0]\n",
      " [   0    0  455    2    0    0    0    0    0    0]\n",
      " [ 114    0    3 2285    7    0  194   80  142  100]\n",
      " [   0    0    0    4  210    0    0    0    0   15]\n",
      " [   0    3    0    1    0  400    0    0    0    0]\n",
      " [   0    0    0   47    0    0 5219    0    0    0]\n",
      " [   3    0    0  198    0    0    0 9376    0    0]\n",
      " [   0    0    0  377    0    0    0    0 6424    0]\n",
      " [   0    0    0  135   63    0    0    1    0 8230]]\n",
      "Accouracy:  0.95662100456621\n",
      "Averages: \n",
      "Precision: 0.9294557107180337\n",
      "Recall: 0.952233908308963\n",
      "F1 score: 0.9394297523196398\n",
      "Category 0:\n",
      "\tPrecision: 0.9866969869243889\n",
      "\tRecall: 0.9553060325847644\n",
      "\tF1 score: 0.9707478046870631\n",
      "Category 1:\n",
      "\tPrecision: 0.9952076677316294\n",
      "\tRecall: 0.9920382165605095\n",
      "\tF1 score: 0.9936204146730463\n",
      "Category 2:\n",
      "\tPrecision: 0.9934497816593887\n",
      "\tRecall: 0.9956236323851203\n",
      "\tF1 score: 0.994535519125683\n",
      "Category 3:\n",
      "\tPrecision: 0.6613603473227206\n",
      "\tRecall: 0.7811965811965812\n",
      "\tF1 score: 0.7163009404388714\n",
      "Category 4:\n",
      "\tPrecision: 0.75\n",
      "\tRecall: 0.9170305676855895\n",
      "\tF1 score: 0.8251473477406679\n",
      "Category 5:\n",
      "\tPrecision: 0.9876543209876543\n",
      "\tRecall: 0.9900990099009901\n",
      "\tF1 score: 0.9888751545117429\n",
      "Category 6:\n",
      "\tPrecision: 0.9641603547016442\n",
      "\tRecall: 0.9910748195974174\n",
      "\tF1 score: 0.9774323438524207\n",
      "Category 7:\n",
      "\tPrecision: 0.991434915935286\n",
      "\tRecall: 0.9790122167693432\n",
      "\tF1 score: 0.9851844068508984\n",
      "Category 8:\n",
      "\tPrecision: 0.9783734389278099\n",
      "\tRecall: 0.9445669754447875\n",
      "\tF1 score: 0.9611730380788509\n",
      "Category 9:\n",
      "\tPrecision: 0.9862192929898143\n",
      "\tRecall: 0.9763910309645272\n",
      "\tF1 score: 0.9812805532371527\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886eb624",
   "metadata": {},
   "source": [
    "# 4. Kernel SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9704f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  1130.1083681583405 seconds\n",
      "peak memory: 5186.91 MiB, increment: 3000.19 MiB\n"
     ]
    }
   ],
   "source": [
    "def train_model_svm_p(X, y):\n",
    "    svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    svc_rbf.fit(X, y)\n",
    "    #end time here\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return svc_rbf\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model_svm_p(train_s, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f0f46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8993    0    0   91    0    0    0    0    0    0]\n",
      " [   0  628    0    0    0    0    0    0    0    0]\n",
      " [   0    0  457    0    0    0    0    0    0    0]\n",
      " [  31    0    3 2640    7    0  128   29   72   15]\n",
      " [   0    0    0    2  212    0    0    0    0   15]\n",
      " [   0    3    0    0    0  401    0    0    0    0]\n",
      " [   0    2    0    0    0    0 5264    0    0    0]\n",
      " [   0    0    0   59    0    0    0 9518    0    0]\n",
      " [   0    0    0  146    0    0    0    0 6655    0]\n",
      " [   0    0    0   12   92    0    0    0    0 8325]]\n",
      "Accouracy:  0.9838584474885844\n",
      "Averages: \n",
      "Precision: 0.9517661086261833\n",
      "Recall: 0.9770538763808116\n",
      "F1 score: 0.9625310405735865\n",
      "Category 0:\n",
      "\tPrecision: 0.9965647163120568\n",
      "\tRecall: 0.9899823866138265\n",
      "\tF1 score: 0.9932626463441574\n",
      "Category 1:\n",
      "\tPrecision: 0.9921011058451816\n",
      "\tRecall: 1.0\n",
      "\tF1 score: 0.9960348929421095\n",
      "Category 2:\n",
      "\tPrecision: 0.9934782608695653\n",
      "\tRecall: 1.0\n",
      "\tF1 score: 0.9967284623773174\n",
      "Category 3:\n",
      "\tPrecision: 0.8949152542372881\n",
      "\tRecall: 0.9025641025641026\n",
      "\tF1 score: 0.8987234042553192\n",
      "Category 4:\n",
      "\tPrecision: 0.6816720257234726\n",
      "\tRecall: 0.925764192139738\n",
      "\tF1 score: 0.7851851851851852\n",
      "Category 5:\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 0.9925742574257426\n",
      "\tF1 score: 0.9962732919254659\n",
      "Category 6:\n",
      "\tPrecision: 0.9762611275964391\n",
      "\tRecall: 0.9996202050892518\n",
      "\tF1 score: 0.9878025896040533\n",
      "Category 7:\n",
      "\tPrecision: 0.9969623965643658\n",
      "\tRecall: 0.9938394069123943\n",
      "\tF1 score: 0.9953984522066514\n",
      "Category 8:\n",
      "\tPrecision: 0.9892968633863535\n",
      "\tRecall: 0.9785325687398911\n",
      "\tF1 score: 0.9838852749852157\n",
      "Category 9:\n",
      "\tPrecision: 0.9964093357271095\n",
      "\tRecall: 0.98766164432317\n",
      "\tF1 score: 0.9920162059103909\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a66e3",
   "metadata": {},
   "source": [
    "# 5. Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b363502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  1.1128098964691162 seconds\n",
      "peak memory: 4161.11 MiB, increment: 2812.48 MiB\n"
     ]
    }
   ],
   "source": [
    "def train_model_nb(X, y):\n",
    "    nb = GaussianNB()\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    nb.fit(X, y)\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return nb\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model_nb(train_s, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104de126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8188    0    0  737    0    0    5  128    2   24]\n",
      " [   0  623    0    0    0    0    0    0    0    5]\n",
      " [   0    0  456    0    0    0    0    1    0    0]\n",
      " [ 147    0    3 1860    0    0  293  182  223  217]\n",
      " [   0    0    0    0  202    0    0    0    0   27]\n",
      " [   0    3    0    0    0  401    0    0    0    0]\n",
      " [  23    0    0    2    0    0 5212   29    0    0]\n",
      " [  98    0    0  272    0    0    0 8927   93  187]\n",
      " [   0    0    0  340    0    0    1    8 6452    0]\n",
      " [ 146    0    0  241    0    0    0    8   46 7988]]\n",
      "Accouracy:  0.9202968036529681\n",
      "Averages: \n",
      "Precision: 0.9278902143252489\n",
      "Recall: 0.9220022073452974\n",
      "F1 score: 0.9240315769421122\n",
      "Category 0:\n",
      "\tPrecision: 0.9518716577540107\n",
      "\tRecall: 0.9013650374284456\n",
      "\tF1 score: 0.9259301142146331\n",
      "Category 1:\n",
      "\tPrecision: 0.9952076677316294\n",
      "\tRecall: 0.9920382165605095\n",
      "\tF1 score: 0.9936204146730463\n",
      "Category 2:\n",
      "\tPrecision: 0.9934640522875817\n",
      "\tRecall: 0.9978118161925602\n",
      "\tF1 score: 0.9956331877729258\n",
      "Category 3:\n",
      "\tPrecision: 0.5388180764774044\n",
      "\tRecall: 0.6358974358974359\n",
      "\tF1 score: 0.5833464011290576\n",
      "Category 4:\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 0.8820960698689956\n",
      "\tF1 score: 0.9373549883990718\n",
      "Category 5:\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 0.9925742574257426\n",
      "\tF1 score: 0.9962732919254659\n",
      "Category 6:\n",
      "\tPrecision: 0.9457448738885864\n",
      "\tRecall: 0.9897455374097988\n",
      "\tF1 score: 0.9672450589217778\n",
      "Category 7:\n",
      "\tPrecision: 0.9616503285575784\n",
      "\tRecall: 0.9321290592043437\n",
      "\tF1 score: 0.946659597030753\n",
      "Category 8:\n",
      "\tPrecision: 0.9465962441314554\n",
      "\tRecall: 0.9486840170563152\n",
      "\tF1 score: 0.9476389806859072\n",
      "Category 9:\n",
      "\tPrecision: 0.9455492424242424\n",
      "\tRecall: 0.9476806264088267\n",
      "\tF1 score: 0.9466137346684836\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a808d3",
   "metadata": {},
   "source": [
    "# 6. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45170e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  28.11222505569458 seconds\n",
      "peak memory: 4384.69 MiB, increment: 900.23 MiB\n"
     ]
    }
   ],
   "source": [
    "def train_model_dt(X, y):\n",
    "    dt = DecisionTreeClassifier()\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    dt.fit(X, y)\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return dt\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model_dt(train_s, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "305de430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9039    0    0   45    0    0    0    0    0    0]\n",
      " [   0  621    0    0    0    7    0    0    0    0]\n",
      " [   0    0  447   10    0    0    0    0    0    0]\n",
      " [  27    0    2 2662    1    0   86   65   80    2]\n",
      " [   0    0    0    3  205    0    0    0    0   21]\n",
      " [   0    3    0    0    0  401    0    0    0    0]\n",
      " [   0    0    0  100    0    0 5166    0    0    0]\n",
      " [   0    1    0   55    0    0    0 9521    0    0]\n",
      " [   0    0    0   76    0    0    0    0 6725    0]\n",
      " [   0    0    0    2   26    0    0    0    0 8401]]\n",
      "Accouracy:  0.986027397260274\n",
      "Averages: \n",
      "Precision: 0.971644567766185\n",
      "Recall: 0.9720540353960211\n",
      "F1 score: 0.9718325092059994\n",
      "Category 0:\n",
      "\tPrecision: 0.9970218398411648\n",
      "\tRecall: 0.9950462351387054\n",
      "\tF1 score: 0.9960330578512396\n",
      "Category 1:\n",
      "\tPrecision: 0.9936\n",
      "\tRecall: 0.9888535031847133\n",
      "\tF1 score: 0.9912210694333599\n",
      "Category 2:\n",
      "\tPrecision: 0.9955456570155902\n",
      "\tRecall: 0.9781181619256017\n",
      "\tF1 score: 0.9867549668874172\n",
      "Category 3:\n",
      "\tPrecision: 0.9014561462919065\n",
      "\tRecall: 0.9100854700854701\n",
      "\tF1 score: 0.9057502551888398\n",
      "Category 4:\n",
      "\tPrecision: 0.8836206896551724\n",
      "\tRecall: 0.8951965065502183\n",
      "\tF1 score: 0.8893709327548807\n",
      "Category 5:\n",
      "\tPrecision: 0.9828431372549019\n",
      "\tRecall: 0.9925742574257426\n",
      "\tF1 score: 0.9876847290640395\n",
      "Category 6:\n",
      "\tPrecision: 0.9836252856054837\n",
      "\tRecall: 0.9810102544625902\n",
      "\tF1 score: 0.9823160296634341\n",
      "Category 7:\n",
      "\tPrecision: 0.9932192781139161\n",
      "\tRecall: 0.9941526574083742\n",
      "\tF1 score: 0.9936857485779887\n",
      "Category 8:\n",
      "\tPrecision: 0.988243938280676\n",
      "\tRecall: 0.9888251727687105\n",
      "\tF1 score: 0.9885344700867265\n",
      "Category 9:\n",
      "\tPrecision: 0.997269705603039\n",
      "\tRecall: 0.9966781350100843\n",
      "\tF1 score: 0.996973832552068\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34cab0",
   "metadata": {},
   "source": [
    "# 7. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28c54bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  150.1497302055359 seconds\n",
      "peak memory: 4620.59 MiB, increment: 1081.88 MiB\n"
     ]
    }
   ],
   "source": [
    "def train_model_rf(X, y):\n",
    "    rf = RandomForestClassifier()\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return rf\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model_rf(train_s, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33137bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9065    0    0   19    0    0    0    0    0    0]\n",
      " [   0  624    0    0    0    0    0    0    0    4]\n",
      " [   0    0  457    0    0    0    0    0    0    0]\n",
      " [  60    0    3 2513    0    0  132   39  142   36]\n",
      " [   0    0    0    1  202    0    0    0    0   26]\n",
      " [   0    3    0    0    0  401    0    0    0    0]\n",
      " [   0    1    0    1    0    0 5264    0    0    0]\n",
      " [   0    0    0   56    0    0    0 9521    0    0]\n",
      " [   0    0    0   14    0    0    0    0 6787    0]\n",
      " [   0    0    0    8    0    0    0    0    0 8421]]\n",
      "Accouracy:  0.9875570776255708\n",
      "Averages: \n",
      "Precision: 0.9885819260847802\n",
      "Recall: 0.9716119847489372\n",
      "F1 score: 0.9794091817636522\n",
      "Category 0:\n",
      "\tPrecision: 0.9934246575342466\n",
      "\tRecall: 0.9979084103918978\n",
      "\tF1 score: 0.995661486078313\n",
      "Category 1:\n",
      "\tPrecision: 0.9936305732484076\n",
      "\tRecall: 0.9936305732484076\n",
      "\tF1 score: 0.9936305732484076\n",
      "Category 2:\n",
      "\tPrecision: 0.9934782608695653\n",
      "\tRecall: 1.0\n",
      "\tF1 score: 0.9967284623773174\n",
      "Category 3:\n",
      "\tPrecision: 0.9620980091883614\n",
      "\tRecall: 0.8591452991452991\n",
      "\tF1 score: 0.9077117572692794\n",
      "Category 4:\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 0.8820960698689956\n",
      "\tF1 score: 0.9373549883990718\n",
      "Category 5:\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 0.9925742574257426\n",
      "\tF1 score: 0.9962732919254659\n",
      "Category 6:\n",
      "\tPrecision: 0.9755374351371386\n",
      "\tRecall: 0.9996202050892518\n",
      "\tF1 score: 0.9874320015006566\n",
      "Category 7:\n",
      "\tPrecision: 0.9959205020920502\n",
      "\tRecall: 0.9941526574083742\n",
      "\tF1 score: 0.9950357945341485\n",
      "Category 8:\n",
      "\tPrecision: 0.9795064222831578\n",
      "\tRecall: 0.9979414791942361\n",
      "\tF1 score: 0.9886380189366352\n",
      "Category 9:\n",
      "\tPrecision: 0.9922234004948746\n",
      "\tRecall: 0.9990508957171669\n",
      "\tF1 score: 0.9956254433672264\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74b424",
   "metadata": {},
   "source": [
    "# 8. AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8f5c6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  152.28145933151245 seconds\n",
      "peak memory: 4601.19 MiB, increment: 2378.11 MiB\n"
     ]
    }
   ],
   "source": [
    "def train_model_ada(X, y):\n",
    "    abc = AdaBoostClassifier()\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    abc.fit(train_s, Y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return abc\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model_ada(train_s, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c8f4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  499    0    0 8585    0    0    0    0    0]\n",
      " [   0  199  211    0  218    0    0    0    0    0]\n",
      " [   0    0  457    0    0    0    0    0    0    0]\n",
      " [   0  360  751    0 1814    0    0    0    0    0]\n",
      " [   0    0    0    0  229    0    0    0    0    0]\n",
      " [   0  404    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1092    0 4174    0    0    0    0    0]\n",
      " [   0    0 9577    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0 6801    0    0    0    0    0]\n",
      " [   0    0    0    0 8429    0    0    0    0    0]]\n",
      "Accouracy:  0.020205479452054795\n",
      "Averages: \n",
      "Precision: 0.018149124769758704\n",
      "Recall: 0.23168789808917198\n",
      "F1 score: 0.02783150739679518\n",
      "Category 0:\n",
      "\tPrecision: 0.0\n",
      "\tRecall: 0.0\n",
      "\tF1 score: 0.0\n",
      "Category 1:\n",
      "\tPrecision: 0.13611491108071136\n",
      "\tRecall: 0.31687898089171973\n",
      "\tF1 score: 0.19043062200956937\n",
      "Category 2:\n",
      "\tPrecision: 0.037806088682991394\n",
      "\tRecall: 1.0\n",
      "\tF1 score: 0.07285771223595057\n",
      "Category 3:\n",
      "\tPrecision: 0.0\n",
      "\tRecall: 0.0\n",
      "\tF1 score: 0.0\n",
      "Category 4:\n",
      "\tPrecision: 0.007570247933884298\n",
      "\tRecall: 1.0\n",
      "\tF1 score: 0.015026739722431837\n",
      "Category 5:\n",
      "\tPrecision: 0.0\n",
      "\tRecall: 0.0\n",
      "\tF1 score: 0.0\n",
      "Category 6:\n",
      "\tPrecision: 0.0\n",
      "\tRecall: 0.0\n",
      "\tF1 score: 0.0\n",
      "Category 7:\n",
      "\tPrecision: 0.0\n",
      "\tRecall: 0.0\n",
      "\tF1 score: 0.0\n",
      "Category 8:\n",
      "\tPrecision: 0.0\n",
      "\tRecall: 0.0\n",
      "\tF1 score: 0.0\n",
      "Category 9:\n",
      "\tPrecision: 0.0\n",
      "\tRecall: 0.0\n",
      "\tF1 score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78040cc0",
   "metadata": {},
   "source": [
    "# 9. Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c67ed875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:808: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  2.524500846862793 seconds\n",
      "peak memory: 4654.77 MiB, increment: 1137.39 MiB\n"
     ]
    }
   ],
   "source": [
    "def train_model_qda(X, y):\n",
    "    qda = QuadraticDiscriminantAnalysis()\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    qda.fit(train_s, Y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return qda\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model_qda(train_s, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4badd3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8565    0    0  519    0    0    0    0    0    0]\n",
      " [   0  623    0    0    0    0    0    0    0    5]\n",
      " [   0    0  450    3    0    0    0    4    0    0]\n",
      " [  75    0    3 2248   10    0  256   83  138  112]\n",
      " [   0    0    0    0  223    0    0    0    0    6]\n",
      " [   0    3    0    0    0  401    0    0    0    0]\n",
      " [   1    0    0    9    0    0 5255    1    0    0]\n",
      " [   0    0    0  199    0    0    0 9378    0    0]\n",
      " [   0    0    0  375    0    0    0    0 6426    0]\n",
      " [   0    0    0  129  120    0    0    0    0 8180]]\n",
      "Accouracy:  0.9531735159817352\n",
      "Averages: \n",
      "Precision: 0.9165537224307788\n",
      "Recall: 0.9546960258585119\n",
      "F1 score: 0.9312928969419045\n",
      "Category 0:\n",
      "\tPrecision: 0.9912047216757319\n",
      "\tRecall: 0.9428665785997358\n",
      "\tF1 score: 0.9664315937940762\n",
      "Category 1:\n",
      "\tPrecision: 0.9952076677316294\n",
      "\tRecall: 0.9920382165605095\n",
      "\tF1 score: 0.9936204146730463\n",
      "Category 2:\n",
      "\tPrecision: 0.9933774834437086\n",
      "\tRecall: 0.9846827133479212\n",
      "\tF1 score: 0.9890109890109889\n",
      "Category 3:\n",
      "\tPrecision: 0.6456059735784032\n",
      "\tRecall: 0.7685470085470085\n",
      "\tF1 score: 0.7017324800998908\n",
      "Category 4:\n",
      "\tPrecision: 0.6317280453257791\n",
      "\tRecall: 0.9737991266375546\n",
      "\tF1 score: 0.7663230240549829\n",
      "Category 5:\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 0.9925742574257426\n",
      "\tF1 score: 0.9962732919254659\n",
      "Category 6:\n",
      "\tPrecision: 0.9535474505534386\n",
      "\tRecall: 0.9979111279908849\n",
      "\tF1 score: 0.9752250162382853\n",
      "Category 7:\n",
      "\tPrecision: 0.9907035706739912\n",
      "\tRecall: 0.9792210504333299\n",
      "\tF1 score: 0.984928845244972\n",
      "Category 8:\n",
      "\tPrecision: 0.9789762340036563\n",
      "\tRecall: 0.9448610498456109\n",
      "\tF1 score: 0.9616161616161616\n",
      "Category 9:\n",
      "\tPrecision: 0.98518607732145\n",
      "\tRecall: 0.9704591291968205\n",
      "\tF1 score: 0.9777671527611761\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b0508b",
   "metadata": {},
   "source": [
    "# 10. MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6d2c6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  261.6060299873352 seconds\n",
      "peak memory: 5120.50 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "def train_model_mlp(X, y):\n",
    "    mlp = MLPClassifier()\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    mlp.fit(train_s, Y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return mlp\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model_mlp(train_s, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4d64aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9048    0    0   34    0    0    1    0    1    0]\n",
      " [   0  619    0    0    0    3    0    0    0    6]\n",
      " [   0    0  456    1    0    0    0    0    0    0]\n",
      " [  51    0    3 2522    3    0  122   85  116   23]\n",
      " [   0    0    0    4  205    0    0    0    0   20]\n",
      " [   0    3    0    0    0  401    0    0    0    0]\n",
      " [   0    1    0   82    0    0 5181    2    0    0]\n",
      " [   0    0    0   68    0    0    0 9508    1    0]\n",
      " [   0    0    0   91    0    0    1    1 6708    0]\n",
      " [   1    0    0   22   26    0    0    0    1 8379]]\n",
      "Accouracy:  0.982351598173516\n",
      "Averages: \n",
      "Precision: 0.9687241938395885\n",
      "Recall: 0.9686558173790292\n",
      "F1 score: 0.9686486665048817\n",
      "Category 0:\n",
      "\tPrecision: 0.9942857142857143\n",
      "\tRecall: 0.9960369881109643\n",
      "\tF1 score: 0.9951605807303124\n",
      "Category 1:\n",
      "\tPrecision: 0.9935794542536116\n",
      "\tRecall: 0.9856687898089171\n",
      "\tF1 score: 0.9896083133493205\n",
      "Category 2:\n",
      "\tPrecision: 0.9934640522875817\n",
      "\tRecall: 0.9978118161925602\n",
      "\tF1 score: 0.9956331877729258\n",
      "Category 3:\n",
      "\tPrecision: 0.8930594900849859\n",
      "\tRecall: 0.8622222222222222\n",
      "\tF1 score: 0.8773699773873718\n",
      "Category 4:\n",
      "\tPrecision: 0.8760683760683761\n",
      "\tRecall: 0.8951965065502183\n",
      "\tF1 score: 0.8855291576673866\n",
      "Category 5:\n",
      "\tPrecision: 0.9925742574257426\n",
      "\tRecall: 0.9925742574257426\n",
      "\tF1 score: 0.9925742574257426\n",
      "Category 6:\n",
      "\tPrecision: 0.9766258246936852\n",
      "\tRecall: 0.9838587162932017\n",
      "\tF1 score: 0.9802289281997918\n",
      "Category 7:\n",
      "\tPrecision: 0.9908295122967903\n",
      "\tRecall: 0.9927952385924611\n",
      "\tF1 score: 0.9918114014499556\n",
      "Category 8:\n",
      "\tPrecision: 0.9825692104877691\n",
      "\tRecall: 0.9863255403617115\n",
      "\tF1 score: 0.9844437921925447\n",
      "Category 9:\n",
      "\tPrecision: 0.9941860465116279\n",
      "\tRecall: 0.9940680982322933\n",
      "\tF1 score: 0.9941270688734649\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8224592",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e80180c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chandrikasaha/opt/anaconda3/envs/tf_keras/lib/python3.6/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to train the model:  7040.160327911377 seconds\n",
      "peak memory: 5252.14 MiB, increment: 655.72 MiB\n"
     ]
    }
   ],
   "source": [
    "def train_model_gradientBoost(X, y):\n",
    "    g = GradientBoostingClassifier()\n",
    "    # Train model and time it\n",
    "    start_time = time.time()\n",
    "    g.fit(X, y)\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate time taken to train\n",
    "    time_taken = end_time - start_time\n",
    "    print(\"Time taken to train the model: \", time_taken, \"seconds\")\n",
    "    return g\n",
    "\n",
    "# train the model and measure the memory usage\n",
    "%memit lr = train_model_gradientBoost(train_s, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91f260e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9051    0    0   33    0    0    0    0    0    0]\n",
      " [   0  628    0    0    0    0    0    0    0    0]\n",
      " [   0    0  455    0    0    0    1    1    0    0]\n",
      " [  40    0    3 2563    0    0  114   46  113   46]\n",
      " [   0    0    0    2  208    0    0    0    0   19]\n",
      " [   0    3    0    0    0  401    0    0    0    0]\n",
      " [   0    0    0    7    0    0 5259    0    0    0]\n",
      " [   0    0    0   53    0    0    0 9524    0    0]\n",
      " [   0    0    0   98    0    0    0    0 6703    0]\n",
      " [   0    0    0   11   24    0    0    0    0 8394]]\n",
      "Accouracy:  0.9859817351598174\n",
      "Averages: \n",
      "Precision: 0.975654837486742\n",
      "Recall: 0.9743676037222331\n",
      "F1 score: 0.9749252427071653\n",
      "Category 0:\n",
      "\tPrecision: 0.99560004399956\n",
      "\tRecall: 0.9963672391017173\n",
      "\tF1 score: 0.9959834938101788\n",
      "Category 1:\n",
      "\tPrecision: 0.9952456418383518\n",
      "\tRecall: 1.0\n",
      "\tF1 score: 0.9976171564733916\n",
      "Category 2:\n",
      "\tPrecision: 0.9934497816593887\n",
      "\tRecall: 0.9956236323851203\n",
      "\tF1 score: 0.994535519125683\n",
      "Category 3:\n",
      "\tPrecision: 0.926273942898446\n",
      "\tRecall: 0.8762393162393163\n",
      "\tF1 score: 0.9005621925509488\n",
      "Category 4:\n",
      "\tPrecision: 0.896551724137931\n",
      "\tRecall: 0.9082969432314411\n",
      "\tF1 score: 0.9023861171366595\n",
      "Category 5:\n",
      "\tPrecision: 1.0\n",
      "\tRecall: 0.9925742574257426\n",
      "\tF1 score: 0.9962732919254659\n",
      "Category 6:\n",
      "\tPrecision: 0.978600669892073\n",
      "\tRecall: 0.9986707178123813\n",
      "\tF1 score: 0.9885338345864662\n",
      "Category 7:\n",
      "\tPrecision: 0.9950893323581653\n",
      "\tRecall: 0.9944659079043542\n",
      "\tF1 score: 0.9947775224566535\n",
      "Category 8:\n",
      "\tPrecision: 0.9834213615023474\n",
      "\tRecall: 0.985590354359653\n",
      "\tF1 score: 0.9845046632885364\n",
      "Category 9:\n",
      "\tPrecision: 0.9923158765811562\n",
      "\tRecall: 0.9958476687626053\n",
      "\tF1 score: 0.9940786357176695\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = lr.predict(test_s)\n",
    "cm_lr = confusion_matrix(Y_test, y_pred_lr)\n",
    "print(cm_lr)\n",
    "acc_lr = accuracy_score(Y_test, y_pred_lr)\n",
    "print (\"Accouracy: \", acc_lr)\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(Y_test, y_pred_lr, average=None)\n",
    "\n",
    "print(\"Averages: \")\n",
    "print(\"Precision:\", mean(precision))\n",
    "print(\"Recall:\", mean(recall))\n",
    "print(\"F1 score:\", mean(f1_score))\n",
    "# print the metrics for each category\n",
    "for i, label in enumerate(lr.classes_):\n",
    "    print(f\"Category {label}:\")\n",
    "    print(f\"\\tPrecision: {precision[i]}\")\n",
    "    print(f\"\\tRecall: {recall[i]}\")\n",
    "    print(f\"\\tF1 score: {f1_score[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf92b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
